{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz_file = bz2.BZ2File('D:/Installation/1305_800230_compressed_test.ft.txt.bz2/test.ft.txt.bz2')\n",
    "file_name_model_latest_version = 'Model/model_latest_version.pt'\n",
    "line_list = bz_file.readlines()\n",
    "training_data_truncation = 20\n",
    "training_data = line_list[0: training_data_truncation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_data = [None for i in range(0, len(training_data))]\n",
    "training_output_data = [None for i in range(0, len(training_data))]\n",
    "\n",
    "training_output_data = [str(x).split('__label__')[1][0] for x in training_data]\n",
    "training_input_data = [re.sub('[^A-Za-z0-9]+',\" \",str(str(x).split('__label__')[1][2: -4])) for x in training_data]\n",
    "#training_input_data[1]\n",
    "#training_output_data[1]\n",
    "\n",
    "\n",
    "#how to keep \"I'll\" kind of word\n",
    "#sort out words by frequency, and only keep the word with top 5000 frequency, and others to be inside 1 group\n",
    "#how to group same meaning of words together?\n",
    "#similiar meaning of words to be together\n",
    "#transfer it into array format before pushing into final machine learning model\n",
    "#make all as lower letter and remove duplication\n",
    "#s,es etc.\n",
    "#how to keep the scetence structure?\n",
    "#in there a better way to save the matrix with most of the value is 0?\n",
    "#remove word only appear once\n",
    "########why LSTM doesn't allow dict lab format input??\n",
    "#need to understand what is the meaning of LSTM parameter of model input. And each step, what is the input & output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_padding(original_list):\n",
    "    max_len = len(max(original_list, key = len))\n",
    "    new_list = []\n",
    "    for m in range(0, len(original_list)):\n",
    "        for i in range(0, max_len - len(original_list[m])):\n",
    "            original_list[m].append(None)\n",
    "    return original_list, max_len\n",
    "def list_to_array_through_dict(original_list, original_dict):\n",
    "    new_list, list_len = list_padding(original_list)\n",
    "    dict_len = len(original_dict)\n",
    "    array_template = [0 for i in range(list_len)]\n",
    "    #print(array_template)\n",
    "    final_list = []\n",
    "    for item in new_list:\n",
    "        new_list_item = copy.deepcopy(array_template)\n",
    "        for j in range(len(item)):\n",
    "            if item[j] != None:\n",
    "                new_list_item[j] = original_dict[item[j]]\n",
    "        final_list.append(new_list_item)\n",
    "    return final_list\n",
    "\n",
    "def output_list_to_array(output_list):\n",
    "    list_len = len(output_list)\n",
    "    output_len = 2\n",
    "    array_template = [0 for i in range(output_len)]\n",
    "    #print(array_template)\n",
    "    final_list = []\n",
    "    for item in output_list:\n",
    "        new_list_item = copy.deepcopy(array_template)\n",
    "        new_list_item[int(item) - 1] = 1\n",
    "        final_list.append(new_list_item)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "721\n"
     ]
    }
   ],
   "source": [
    "words = set((' '.join(training_input_data)).split())\n",
    "int2word = dict(enumerate(words))\n",
    "vocab_size = len(int2word)\n",
    "\n",
    "word2int = {word: ind for ind, word in int2word.items()}\n",
    "word2int\n",
    "\n",
    "training_input_data_split = [x.split() for x in training_input_data]\n",
    "\n",
    "max_len = len(max(training_input_data_split, key = len))\n",
    "#training_input_data_split\n",
    "print(max_len)\n",
    "\n",
    "#training_input_data_split = [for i in range(max_len - len(x)): x.insert(0, \"\") for x in training_input_data_split]\n",
    "int2word\n",
    "#new_list = list_padding(training_input_data_split)\n",
    "#print(new_list)\n",
    "print(len(int2word))\n",
    "\n",
    "\n",
    "#final_input = torch.Tensor(list_to_array_through_dict(training_input_data_split, word2int))\n",
    "\n",
    "#final_output = torch.Tensor(output_list_to_array(training_output_data))\n",
    "#print(final_output)\n",
    "final_input = np.array(list_to_array_through_dict(training_input_data_split, word2int))\n",
    "final_output = np.array(output_list_to_array(training_output_data))\n",
    "final_input = torch.Tensor(final_input)\n",
    "final_output = torch.Tensor(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 152)\n",
      "torch.Size([20, 152])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(new_list[0][0])\n",
    "kkk = list_to_array_through_dict(training_input_data_split, word2int)\n",
    "kkk = np.array(kkk)\n",
    "print(kkk.shape)\n",
    "\n",
    "mm = final_input.size()\n",
    "print(mm)\n",
    "\n",
    "final_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[341., 437., 595.,  ...,   0.,   0.,   0.],\n",
      "        [676., 208., 293.,  ..., 470., 104., 329.],\n",
      "        [ 12., 569., 484.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [647., 208., 141.,  ...,   0.,   0.,   0.],\n",
      "        [330.,  97.,  39.,  ...,   0.,   0.,   0.],\n",
      "        [ 19., 208., 293.,  ...,   0.,   0.,   0.]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(final_input)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.fc2 = nn.Linear(max_len * 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        #print(batch_size)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        #print(out)\n",
    "        #out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (epoch: 5): 0.24732181\n",
      "Loss (epoch: 10): 0.24752961\n",
      "Loss (epoch: 15): 0.24761406\n",
      "Loss (epoch: 20): 0.24759488\n",
      "Loss (epoch: 25): 0.24731971\n",
      "Loss (epoch: 30): 0.24720158\n",
      "Loss (epoch: 35): 0.2470372\n",
      "Loss (epoch: 40): 0.24687463\n"
     ]
    }
   ],
   "source": [
    "#def training_model():\n",
    "learning_rate = 0.3\n",
    "epoch_size = 40\n",
    "steps_for_printing_out_loss = 2\n",
    "\n",
    "output_size = 2\n",
    "#output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "LSTM_model_training_WIP = LSTM_model(vocab_size, output_size = output_size, embedding_dim = embedding_dim, hidden_dim = hidden_dim, n_layers = n_layers)\n",
    "LSTM_model_training_WIP.to(device)\n",
    "#RNN_model.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(LSTM_model_training_WIP.parameters(), lr = learning_rate)\n",
    "input = final_input\n",
    "target = final_output\n",
    "for i in range(1, epoch_size + 1):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = LSTM_model_training_WIP(input)\n",
    "    #print(output)\n",
    "    #print(output.size())\n",
    "    #print(hidden)\n",
    "    #print(hidden.size)\n",
    "    loss = loss_functioin(output, target)\n",
    "    loss.backward()\n",
    "    if i % (steps_for_printing_out_loss) == 0:\n",
    "        print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save({'state_dict': LSTM_model_training_WIP.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_latest_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
