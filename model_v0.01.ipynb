{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz_file = bz2.BZ2File('D:/Installation/1305_800230_compressed_test.ft.txt.bz2/test.ft.txt.bz2')\n",
    "file_name_model_latest_version = 'Model/model_latest_version.pt'\n",
    "line_list = bz_file.readlines()\n",
    "training_data_truncation = 2\n",
    "training_data = line_list[0: training_data_truncation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_data = [None for i in range(0, len(training_data))]\n",
    "training_output_data = [None for i in range(0, len(training_data))]\n",
    "\n",
    "training_output_data = [str(x).split('__label__')[1][0] for x in training_data]\n",
    "training_input_data = [re.sub('[^A-Za-z0-9]+',\" \",str(str(x).split('__label__')[1][2: -4])) for x in training_data]\n",
    "#training_input_data[1]\n",
    "#training_output_data[1]\n",
    "\n",
    "\n",
    "#how to keep \"I'll\" kind of word\n",
    "#sort out words by frequency, and only keep the word with top 5000 frequency, and others to be inside 1 group\n",
    "#how to group same meaning of words together?\n",
    "#similiar meaning of words to be together\n",
    "#transfer it into array format before pushing into final machine learning model\n",
    "#make all as lower letter and remove duplication\n",
    "#s,es etc.\n",
    "#how to keep the scetence structure?\n",
    "#in there a better way to save the matrix with most of the value is 0?\n",
    "#remove word only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_padding(original_list):\n",
    "    max_len = len(max(original_list, key = len))\n",
    "    new_list = []\n",
    "    for m in range(0, len(original_list)):\n",
    "        for i in range(0, max_len - len(original_list[m])):\n",
    "            original_list[m].append(None)\n",
    "    return original_list, max_len\n",
    "def list_to_array_through_dict(original_list, original_dict):\n",
    "    new_list, list_len = list_padding(original_list)\n",
    "    dict_len = len(original_dict)\n",
    "    array_template = [[0 for i in range(dict_len)] for j in range(list_len)]\n",
    "    #print(array_template)\n",
    "    final_list = []\n",
    "    for item in new_list:\n",
    "        new_list_item = copy.deepcopy(array_template)\n",
    "        for j in range(len(item)):\n",
    "            if item[j] != None:\n",
    "                new_list_item[j][original_dict[item[j]]] = 1\n",
    "        final_list.append(new_list_item)\n",
    "    return final_list\n",
    "\n",
    "def output_list_to_array(output_list):\n",
    "    list_len = len(output_list)\n",
    "    output_len = 2\n",
    "    array_template = [0 for i in range(output_len)]\n",
    "    #print(array_template)\n",
    "    final_list = []\n",
    "    for item in output_list:\n",
    "        new_list_item = copy.deepcopy(array_template)\n",
    "        new_list_item[int(item) - 1] = 1\n",
    "        final_list.append(new_list_item)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "words = set((' '.join(training_input_data)).split())\n",
    "int2word = dict(enumerate(words))\n",
    "vocab_size = len(int2word)\n",
    "\n",
    "word2int = {word: ind for ind, word in int2word.items()}\n",
    "word2int\n",
    "\n",
    "training_input_data_split = [x.split() for x in training_input_data]\n",
    "\n",
    "max_len = len(max(training_input_data_split, key = len))\n",
    "#training_input_data_split\n",
    "print(max_len)\n",
    "\n",
    "#training_input_data_split = [for i in range(max_len - len(x)): x.insert(0, \"\") for x in training_input_data_split]\n",
    "int2word\n",
    "#new_list = list_padding(training_input_data_split)\n",
    "#print(new_list)\n",
    "print(len(int2word))\n",
    "\n",
    "\n",
    "#final_input = torch.Tensor(list_to_array_through_dict(training_input_data_split, word2int))\n",
    "\n",
    "#final_output = torch.Tensor(output_list_to_array(training_output_data))\n",
    "#print(final_output)\n",
    "final_input = np.array(list_to_array_through_dict(training_input_data_split, word2int))\n",
    "final_output = np.array(output_list_to_array(training_output_data))\n",
    "final_input = torch.Tensor(final_input)\n",
    "final_output = torch.Tensor(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 152, 161)\n",
      "torch.Size([2, 152, 161])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(new_list[0][0])\n",
    "kkk = list_to_array_through_dict(training_input_data_split, word2int)\n",
    "kkk = np.array(kkk)\n",
    "print(kkk.shape)\n",
    "\n",
    "mm = final_input.size()\n",
    "print(mm)\n",
    "\n",
    "final_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(final_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        print(batch_size)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        #out = self.sigmoid(out)\n",
    "        \n",
    "        #out = out.view(batch_size, -1)\n",
    "        #print(out)\n",
    "        #out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "tensor([[-0.0150, -0.0197],\n",
      "        [-0.0322,  0.0022],\n",
      "        [-0.0289, -0.0433],\n",
      "        ...,\n",
      "        [-0.0423, -0.0400],\n",
      "        [-0.0786,  0.0417],\n",
      "        [ 0.0402, -0.0878]], grad_fn=<ThAddmmBackward>)\n",
      "torch.Size([24472, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[24472, 2]' is invalid for input of size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-3c0f61bebb07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#print(hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#print(hidden.size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_functioin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msteps_for_printing_out_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[24472, 2]' is invalid for input of size 2"
     ]
    }
   ],
   "source": [
    "#def training_model():\n",
    "learning_rate = 0.3\n",
    "epoch_size = 2\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "output_size = 2\n",
    "#output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "LSTM_model_training_WIP = LSTM_model(vocab_size = vocab_size, output_size = output_size, embedding_dim = embedding_dim, hidden_dim = hidden_dim, n_layers = n_layers)\n",
    "#RNN_model.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(LSTM_model_training_WIP.parameters(), lr = learning_rate)\n",
    "\n",
    "for training_step_no in range(0, training_data_truncation):\n",
    "    input = final_input[training_step_no]\n",
    "    target = final_output[training_step_no]\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = LSTM_model_training_WIP(input)\n",
    "        print(output)\n",
    "        print(output.size())\n",
    "        #print(hidden)\n",
    "        #print(hidden.size)\n",
    "        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1)))\n",
    "        loss.backward()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "        optimizer.step()\n",
    "\n",
    "torch.save({'state_dict': LSTM_model_training_WIP.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_latest_version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
